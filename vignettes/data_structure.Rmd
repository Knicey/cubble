---
title: "data_structure"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{data_structure}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(cubble)
library(tsibble)
library(dplyr)
library(stringr)
library(ozmaps)
library(sf)
```

# Motivation for a new data structure:

```{r}
climate <- climate_monthly %>% head(100)

# build a tsibble
climate_tsibble <- climate %>% 
  mutate(date = yearmonth(date)) %>% 
  as_tsibble(key = c(station, param), index = date)

# build an sf from tsibble - so far so good
climate_sf <- climate_tsibble %>% filter(!is.na(lat)) %>% 
  st_as_sf(coords = c("long", "lat"), crs = st_crs(ozmap_country))
class(climate_sf)

# spatial join returns an tibble/ sf - lose the tsibble structure
a <- climate_sf %>% 
  st_join(ozmap_states, join = st_within)
class(a) 

# turning it to tsibble again loses the sf structure
b <- a %>% as_tsibble(index = date, key = c(station, param))
class(b)
b %>% st_join(ozmap_states, join = st_within)

# hence would be nice to have a data structure that deal with both dimensions
# Building such a data structure would mean dependence on both tsibble and sf 
```

# Existing and similar work:

## Data structure:

-   `spacetime` builds a spatiotemporal object based on `xts` and `sp` class for handling time and spatial dimension. Both classes have been superseded by modern data structure, namely, `tsibble` and `sf`, respectively.

    -   Spacetime: Spatio-Temporal Data in R

-   `stars` package defines the spatiotemporal data as an array and provides operations based on the array structure. This definition fits well with the satellite imagery from remote sensing, where pixels in the image corresponds to the cells in the array.

    -   <https://github.com/r-spatial/stars/>

    -   "Multidimensional Arrays for anlaysing geoscientific data"

    -   "Spatio-temporal change detection from multidimensional arrays: Detecting deforestation from MOBIS time series"

-   `cubelyr` implements a `tbl_cube` class for tidyverse operations on arrays.

    -   <https://github.com/hadley/cubelyr/blob/master/R/cube.R>

-   `sf` implements a flat data structure for handling different geometry types in spatial data and introduces a set of operations based on the geometrical relations.

    -   Simple Features for R: standardized support for spatial vector data

-   `tsibble` implements a data structure for temporal data through defining the `key` and `index` variable

    -   A new tidy data structure to support exploration and modeling of temporal data

## Data wrangling operations:

-   There are also array operations defined in the following paper: **select, scale, reduce, rearrange, and compute**.

    -   Spatio-temporal change detection from multidimensional arrays: Detecting deforestation from MOBIS time series"

-   The package is still in experimental and the operations supported includes **select, rename filter, mutate, arrange, group_by/ungroup, and summarise (no join).**

    -   `cubelyr`

## How my work differs:

-   fixed set of locations + interest in time domain

    -   While satellite imagery can record information on a continuous spatial domain, other spatial information are recorded at a fixed set of locations characterised by latitude, longitude, and other spatial metadata. Examples of this type of data includes climate measures from weather observation stations and river level data recorded by electronic gauges. Analysing this data requires less considerations on the geometry type and map projection while more on how measures in these fixed locations changes across the time domain and whether these changes are related for adjacent locations.

    -   characteristics:

        -   less focus on complex geometry structure (point geometry) and projection

        -   more focus on the time dimension

        -   can be regular or irregular grid

-   A flat data structure for saptiotemporal data built on top of `tibble` means the adoption of tidyverse workflow where the data type and inputs and outputs are predictable and hence operations can be concatenated on top of one another. This workflow enhances the reproducibility of data analysis.


```{r}
water_large
spatio <- water_large %>% select(station, name, lat, long) %>% distinct()

dist <- expand_grid(from = spatio$station, to = spatio$station) %>% 
  left_join(spatio, by = c("from" = "station")) %>% 
  left_join(spatio, by = c("to" = "station")) %>% 
  mutate(dist = sqrt((lat.x - lat.y)^2 + (long.x - long.y)^2)) %>% 
  filter(dist != 0) %>% 
  nest(-c(from, name.x, lat.x, long.x)) %>% 
  mutate(data = map(data, ~.x %>% select(to, dist, name.y) %>% arrange(dist))) 


# cross tabulate the key to create a separate spatio sheet 
water_large %>% 
  # nested calculation of distance on the spatio sheet
  compute(dist = closest(n = 10), on_spatio = TRUE) %>%  
  # dist is on the spatio sheet but can be called by the main sheet 
  # find the distance < 1 for station 221224
  # look at spatial sheet for filtering if key is involved???? 
  filter(dist < 1, station == "221224")
  
```

# A relational data structure

In spatiotemporal data, A single table data structure repeats the recording of time-invariant measures. This significantly increases the computation time and storage needed for those data that are long in the time dimension. Cubble implements a relational data structure with two tables that separately record time-variant and invariant variables for spatiotemporal data. After creating a cubble object, the two main verbs `choose` and `cubble_join` choose the table to work on and join the output results to either the main or item sheet depending on whether the results are time-variant or invariant. All the intermediate syntaxes can be written as usual without being affected.

```{r}
df <- cubble(climate, station, key = c("station" = "id"))

df %>% 
  choose(climate) %>% 
  ... %>% 
  cubble_join()
```

# Nesting

`nest_by` is a verb I recently discovered in dplyr. It is powered by the `rowwise` operation to create a list-column that nests each group into one row. I think it is a nice structure that makes compute station-wise values much easier (see below). However, I think the unnest workflow can be improved a bit further: currently, one need to use both `unnest` + `ungroup` to restore to the initial unnested and ungrouped stage. It would be nice to have these two operations combined.

```{r}
start_end <- climate %>% 
  nest_by(station) %>% 
  mutate(start = min(data$date), 
         end = max(data$date))

count_var <- function(dt){
  dt %>% 
    as_tibble() %>% 
    tidyr::pivot_longer(prcp:tavg, names_to = "datatype", values_to = "value") %>% 
    group_by(datatype) %>% 
    # think of a better way to filter out those block missing
    summarise(missing = sum(value, na.rm = TRUE)) %>% 
    filter(missing != 0) %>% nrow()
}

climate %>% 
  nest_by(station) %>% 
  mutate(var_count = count_var(data))
```

```{r}
start_end %>% 
  tidyr::unnest(data) %>% 
  ungroup()
```

The associated `nest_by()` can join two data frames while keep a nesting structure. With `rowwise`, this is elegant to compute station-wise information. Currently, it doesn't support the nested list to be a tsibble tho.

```{r}
joined <- station %>% nest_join(climate, by = c("id" = "station"))

joined %>% 
  rowwise() %>% 
  mutate(start = min(climate$date))

joined2 <- climate %>% nest_join(station, by = c("station" = "id"))
```

Also, referening to variables in the list-column is not particularly convenient with the current infrastructure. For example,  in an example to add the proportion of presence for `prcp` column, it would be nice if we can drop `data$` when referencing to variable in the nested list.

```{r}
start_end %>% 
  mutate(prcp_missing = sum(is.na(data$prcp) / length(data$prcp)))
```

Also to repeat this calculation for all the variables (`prcp`: `tavg`) in an unnested structure, we can do

```{r}
climate %>% mutate(across(prcp: tavg, ~sum(is.na(.x))/ length(.x)))
```

But this won't work in list-column

```{r eval = FALSE}
start_end %>% mutate(across(data$prcp: data$tavg, ~sum(is.na(.x))/ length(.x)))
# Error: Problem with `mutate()` input `..1`.
# ℹ `..1 = across(data$prcp:data$tavg, ~sum(is.na(.x))/length(.x))`.
# x object of type 'closure' is not subsettable
# ℹ The error occurred in row 1.
# Run `rlang::last_error()` to see where the error occurred.
```

# Long or wide form?

Now I prefer a wide form rather than long form for the main dataset. Long form is easier to operate upon when all the variables require the same operation (i.e. aggregate all the variables by mean) while wide form is more friendly for operations on individual variables. There has been `across()` implemented to batch process variables with the same operation, which I think is rather elegant. Using long form would imply re-define column-wise dplyr verbs in the context of long form (i.e. select/ deselect a variable is somehow a filter, mutate becomes somehow adding rows).



